<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">From Data to Knowledge: The Power of Vector Databases | Brain Blog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" property="og:url" content="https://tchez.dev/blog/vector-database/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="pt_BR"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="twitter:creator" content="@_tchez"><meta data-rh="true" name="twitter:title" content="Brain Blog – AI, Notes &amp; Projects"><meta data-rh="true" name="twitter:description" content="Articles, structured notes and project logs by Marco Antônio."><meta data-rh="true" property="og:title" content="From Data to Knowledge: The Power of Vector Databases | Brain Blog"><meta data-rh="true" name="description" content="In a world flooded with unstructured data, how can we store not just information, but knowledge itself? This article dives into vector databases — a revolutionary approach that enables similarity-based search and semantic understanding. Learn how vectors and embeddings reshape data storage, power advanced AI applications, and mark a shift from traditional databases to knowledge-driven systems."><meta data-rh="true" property="og:description" content="In a world flooded with unstructured data, how can we store not just information, but knowledge itself? This article dives into vector databases — a revolutionary approach that enables similarity-based search and semantic understanding. Learn how vectors and embeddings reshape data storage, power advanced AI applications, and mark a shift from traditional databases to knowledge-driven systems."><meta data-rh="true" name="keywords" content="vector storage,vector database,embeddings,natural language processing,similarity search,ai applications,knowledge storage"><meta data-rh="true" property="og:image" content="https://tchez.dev/img/blog/vector-database/article-og.png"><meta data-rh="true" name="twitter:image" content="https://tchez.dev/img/blog/vector-database/article-og.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-06-12T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/tchez"><meta data-rh="true" property="article:tag" content="rag,ai,programming,article"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://tchez.dev/blog/vector-database/"><link data-rh="true" rel="alternate" href="https://tchez.dev/blog/vector-database/" hreflang="en"><link data-rh="true" rel="alternate" href="https://tchez.dev/pt/blog/vector-database/" hreflang="pt-BR"><link data-rh="true" rel="alternate" href="https://tchez.dev/blog/vector-database/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://tchez.dev/blog/vector-database","mainEntityOfPage":"https://tchez.dev/blog/vector-database","url":"https://tchez.dev/blog/vector-database","headline":"From Data to Knowledge: The Power of Vector Databases","name":"From Data to Knowledge: The Power of Vector Databases","description":"In a world flooded with unstructured data, how can we store not just information, but knowledge itself? This article dives into vector databases — a revolutionary approach that enables similarity-based search and semantic understanding. Learn how vectors and embeddings reshape data storage, power advanced AI applications, and mark a shift from traditional databases to knowledge-driven systems.","datePublished":"2025-06-12T00:00:00.000Z","author":{"@type":"Person","name":"Marco Antônio Martins Porto Netto","description":"Full‑Stack Dev & AI Enthusiast","url":"https://github.com/tchez","image":"https://github.com/tchez.png"},"image":{"@type":"ImageObject","@id":"https://tchez.dev/img/blog/vector-database/article-og.png","url":"https://tchez.dev/img/blog/vector-database/article-og.png","contentUrl":"https://tchez.dev/img/blog/vector-database/article-og.png","caption":"title image for the blog post: From Data to Knowledge: The Power of Vector Databases"},"keywords":["vector storage","vector database","embeddings","natural language processing","similarity search","ai applications","knowledge storage"],"isPartOf":{"@type":"Blog","@id":"https://tchez.dev/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Brain Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Brain Blog Atom Feed"><link rel="stylesheet" href="/assets/css/styles.1f2cd8b8.css">
<script src="/assets/js/runtime~main.42f25405.js" defer="defer"></script>
<script src="/assets/js/main.cef9618d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.png"><link rel="preload" as="image" href="https://github.com/tchez.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Brain Blog Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.png" alt="Brain Blog Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Brain Blog</b></a><a class="navbar__item navbar__link" href="/blog/welcome/">About Brain Blog</a><a class="navbar__item navbar__link" href="/notes/intro/">Notes</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog/">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/blog/vector-database/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/pt/blog/vector-database/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt-BR">Português</a></li></ul></div><a href="https://www.linkedin.com/in/tchez/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://github.com/tchez/brain-blog" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/vector-database/">From Data to Knowledge: The Power of Vector Databases</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/dunder-methods/">Do you know what magic methods are in Python? Hint: You use them every day!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/welcome/">Welcome to Brain Blog 🎉</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">From Data to Knowledge: The Power of Vector Databases</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-06-12T00:00:00.000Z">June 12, 2025</time> · <!-- -->13 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/tchez" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/tchez.png" alt="Marco Antônio Martins Porto Netto"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/tchez" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Marco Antônio Martins Porto Netto</span></a></div><small class="authorTitle_nd0D" title="Full‑Stack Dev &amp; AI Enthusiast">Full‑Stack Dev &amp; AI Enthusiast</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAYAAAD68A/GAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAo0lEQVR4nDWOuwqFQAwF4xMRXwhrYaOCX2An2PghgmIngqWfP5JcbnFgkx0yRzzPQ+P7viUMQ6qqIssyoigiCAJEBNGFAjrEcUxd1wamaWqQ/ukhGYaB67rYto193zmOg+d5eN+X8zy57xvnHFIUBW3b0vc90zSxLAvrujLPM+M40nUdypg6SRKL6lSt0RrWTeSnViDPc8qypGkaeyv0722QCB9tUk3gQY4M9wAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="360"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/article-og.2767017.640.png" srcset="/assets/ideal-img/article-og.2767017.640.png 640w,/assets/ideal-img/article-og.7dd2e07.920.png 920w,/assets/ideal-img/article-og.7d655f3.1200.png 1200w" width="640" height="360"></noscript></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="from-data-to-knowledge-the-power-of-vector-databases">From Data to Knowledge: The Power of Vector Databases<a href="#from-data-to-knowledge-the-power-of-vector-databases" class="hash-link" aria-label="Direct link to From Data to Knowledge: The Power of Vector Databases" title="Direct link to From Data to Knowledge: The Power of Vector Databases">​</a></h2>
<p><strong>In a world flooded with unstructured data, how can we store not just information, but knowledge itself?</strong></p>
<p>This article explores the rise of <strong>vector databases</strong> — a revolutionary technology that enables similarity-based search and semantic understanding. We&#x27;ll break down what vectors are, how they&#x27;re used in <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank" rel="noopener noreferrer">Natural Language Processing (NLP)</a> and how <strong>embeddings</strong> allow for efficient, contextualized representations of information. Finally, we’ll explore real-world applications and why vector databases are becoming essential in modern AI systems.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-rise-of-unstructured-data">The Rise of Unstructured Data<a href="#the-rise-of-unstructured-data" class="hash-link" aria-label="Direct link to The Rise of Unstructured Data" title="Direct link to The Rise of Unstructured Data">​</a></h3>
<p>With the advance of <a href="https://en.wikipedia.org/wiki/Big_Data" target="_blank" rel="noopener noreferrer">Big Data</a>, billions of connected devices generate real-time information in the form of text, images, videos, and more. These unstructured formats don&#x27;t fit well into traditional SQL tables and demand more sophisticated storage solutions. This is where <strong>vector databases</strong> emerge as a game-changing approach, enabling similarity-based searches and unlocking knowledge from complex data.</p>
<p>Before we dive into how vector databases work, let’s first recapitulate the fundamentals of databases.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-database">What Is a Database?<a href="#what-is-a-database" class="hash-link" aria-label="Direct link to What Is a Database?" title="Direct link to What Is a Database?">​</a></h2>
<p>A <strong>database</strong> is, simply put, an organized collection of information that can be efficiently accessed, managed and updated. It acts as a structure that stores and organizes data, making it easy to query and manipulate using specialized software.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="relational-databases">Relational Databases<a href="#relational-databases" class="hash-link" aria-label="Direct link to Relational Databases" title="Direct link to Relational Databases">​</a></h3>
<p><strong>Relational databases</strong> are the most commonly used type. They store data in tables organized into rows and columns — where each row represents a record and each column represents a field or attribute. This model is ideal for structured data such as customer records, bank transactions, or product inventories.</p>
<p>A classic example is a customer database with columns for name, address, phone number and email. Relational databases enable fast and precise queries like “find all customers who made a purchase in the last 30 days,” using a language known as <a href="https://en.wikipedia.org/wiki/SQL" target="_blank" rel="noopener noreferrer">SQL (Structured Query Language)</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="non-relational-databases-nosql">Non-Relational Databases (NoSQL)<a href="#non-relational-databases-nosql" class="hash-link" aria-label="Direct link to Non-Relational Databases (NoSQL)" title="Direct link to Non-Relational Databases (NoSQL)">​</a></h3>
<p>Unlike relational databases, <strong>NoSQL databases</strong> are designed to handle unstructured or semi-structured data, offering more flexibility and scalability. They store information in diverse formats such as JSON documents, key-value pairs or <a href="https://en.wikipedia.org/wiki/Graph" target="_blank" rel="noopener noreferrer">graphs</a>. This makes them better suited for modern applications like social-media platforms or streaming services.</p>
<p>For instance, a document-oriented NoSQL database might store data in JSON format, allowing for more complex and nested data structures — without requiring a rigid schema.</p>
<p>Among the categories of non-relational databases, <strong>vector databases</strong> stand out for their ability to store contextualized data — also known as <strong>knowledge</strong>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vector-databases">Vector Databases<a href="#vector-databases" class="hash-link" aria-label="Direct link to Vector Databases" title="Direct link to Vector Databases">​</a></h3>
<p><strong>Vector databases</strong> introduce a transformative approach tailored to store and retrieve data as <strong>vectors</strong> — mathematical structures that represent information in multiple dimensions. Unlike traditional databases that rely on exact matching, vector databases enable <strong>similarity searches</strong>, which are ideal for retrieving content like texts, images or sounds based on their characteristics.</p>
<p>This brings us to a key question: <em>what is a vector?</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-vector">What Is a Vector?<a href="#what-is-a-vector" class="hash-link" aria-label="Direct link to What Is a Vector?" title="Direct link to What Is a Vector?">​</a></h2>
<p>A <strong>vector</strong> is a structure that stores information across multiple dimensions. In the case of a three-dimensional vector, it has three coordinates — <em>(x, y, z)</em> — that define its position or direction in a 3-D space.</p>
<p>A real-world example is how colors are represented in the <a href="https://en.wikipedia.org/wiki/RGB" target="_blank" rel="noopener noreferrer"><strong>RGB</strong></a> model (Red, Green, Blue). A color is described by three values, each representing the intensity of red, green and blue. For instance, white is <code>[255, 255, 255]</code>, while black is <code>[0, 0, 0]</code>.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAACCAIAAADuA9qHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAASUlEQVR4nAE+AMH/AOKIiOKgn+a5uPLJzMm/rKXNzMrV5a+sqaOkpba2twDooaHr29ru///1v8S4o4R0t7a+z+v89/Ls7+/z9PV5NC7scU7kSwAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="138"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/image-1.5ea0378.640.png" srcset="/assets/ideal-img/image-1.5ea0378.640.png 640w,/assets/ideal-img/image-1.cff2c0a.920.png 920w,/assets/ideal-img/image-1.40e3993.1166.png 1166w" width="640" height="138"></noscript></div>
<br>
<p>This concept extends to <strong>4-D vectors</strong>, such as <a href="https://en.wikipedia.org/wiki/RGBA" target="_blank" rel="noopener noreferrer"><strong>RGBA</strong></a>, where the fourth component “A” stands for <strong>alpha</strong> (transparência). A semi-transparent red poderia ser <code>[255, 0, 0, 0.5]</code>.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAACCAIAAADuA9qHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAPklEQVR4nCXJuwkAIAwFQPffJZUjSFoH8RMQNBCIPkGvvYBzAGzVRVRTKswz522GJ/x21RKj9C5jNBF3/30BuHc4DW+0o5AAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="138"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/image-2.3d33e6c.640.png" srcset="/assets/ideal-img/image-2.3d33e6c.640.png 640w,/assets/ideal-img/image-2.b3e0325.920.png 920w,/assets/ideal-img/image-2.f4578cf.1166.png 1166w" width="640" height="138"></noscript></div>
<br>
<p>Just as colors can be represented by vectors in three or four dimensions, vector databases use vectors to organize items such as words, images and sounds in multidimensional spaces, where the <strong>distance</strong> between vectors indicates similarity.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="representing-words-with-vectors">Representing Words with Vectors<a href="#representing-words-with-vectors" class="hash-link" aria-label="Direct link to Representing Words with Vectors" title="Direct link to Representing Words with Vectors">​</a></h3>
<p>Vectors are widely used to represent more abstract information, such as words. In NLP, converting words into numbers is essential for machines to understand and manipulate them. One of the simplest techniques is <strong>One-Hot Encoding</strong>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="one-hot-encoding">One-Hot Encoding<a href="#one-hot-encoding" class="hash-link" aria-label="Direct link to One-Hot Encoding" title="Direct link to One-Hot Encoding">​</a></h3>
<p>One-Hot Encoding is a technique in which each word is transformed into a vector of zeros and ones. In this vector, each position corresponds to a specific word in the vocabulary, and only one position holds the value &quot;1&quot; — all the others are set to &quot;0&quot;.</p>
<p>Let’s consider a set of four words: dog, cat, bird, and fish. Using One-Hot Encoding, these words would be represented as follows:</p>
<ul>
<li><strong>dog:</strong> [1, 0, 0, 0]</li>
<li><strong>cat:</strong> [0, 1, 0, 0]</li>
<li><strong>bird:</strong> [0, 0, 1, 0]</li>
<li><strong>fish:</strong> [0, 0, 0, 1]</li>
</ul>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAIAAADzBuo/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAgUlEQVR4nBXJSwrCMBAA0Nz/am6KiAiKGwWpNOk0mU+SyUzEt33B3W0aACDinNPdpdWYopn59MBDTmk58gEAzDzaOMfrIz0pkw4N5ia9wg6YUYeaGTKlPWlTdw9m1qS9tveyXXprvXcsCAlu+R57+jcRZS6fsoqIqhJRFVnLl4f8AGwAjxGma5geAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="299"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/image-3.edca5ba.640.png" srcset="/assets/ideal-img/image-3.edca5ba.640.png 640w,/assets/ideal-img/image-3.cf34687.920.png 920w,/assets/ideal-img/image-3.d9affd3.1166.png 1166w" width="640" height="299"></noscript></div>
<br>
<p>Each vector contains only one &quot;1&quot;, indicating the corresponding word, while the &quot;0s&quot; indicate that the other words are not present. Although this approach is simple and easy to implement, it comes with some limitations — especially as the vocabulary begins to grow.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-limitations-of-one-hot-encoding">The Limitations of One-Hot Encoding<a href="#the-limitations-of-one-hot-encoding" class="hash-link" aria-label="Direct link to The Limitations of One-Hot Encoding" title="Direct link to The Limitations of One-Hot Encoding">​</a></h3>
<p>Although simple, One-Hot Encoding doesn&#x27;t capture semantic relationships between words. In the example above, &quot;dog&quot; and &quot;cat&quot; are treated as completely different, even though both are household pets, have four legs, two ears, and a tail. Another drawback appears in scenarios with large vocabularies — say, 10,000 words. In such cases, each word would be represented by a very long vector with a single &quot;1&quot; among 10,000 positions, offering no indication of how the words relate to one another. This inefficiency and lack of semantic information highlight the need for more advanced techniques to represent words as vectors in a meaningful way.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="advanced-representations-embeddings">Advanced Representations: Embeddings<a href="#advanced-representations-embeddings" class="hash-link" aria-label="Direct link to Advanced Representations: Embeddings" title="Direct link to Advanced Representations: Embeddings">​</a></h3>
<p>To overcome these limitations, embeddings were introduced — dense and continuous vector representations of words, where words with similar meanings are placed close to each other in the vector space. Unlike One-Hot Encoding, embeddings capture the meaning of words, positioning related terms — like “dog” and “cat” — near each other in the vector space, since they share common features.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAIAAADzBuo/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAZ0lEQVR4nFWLwRJAMAwF/f8HurnRtIxEUdJM2xjlwDvsYXdeo3UpF44CA0xIzJIkPb65UQpxthShs6ZH0/Zol+o/2UUdWcc9w+AJw+etOns2sLlD3ZpgZjzzL287o/Or6EIBDQZ58wXCY5CUpJmE9gAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="299"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/image-4.8c7f92e.640.png" srcset="/assets/ideal-img/image-4.8c7f92e.640.png 640w,/assets/ideal-img/image-4.cd53446.920.png 920w,/assets/ideal-img/image-4.ab95381.1166.png 1166w" width="640" height="299"></noscript></div>
<br>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-embeddings">What Are Embeddings?<a href="#what-are-embeddings" class="hash-link" aria-label="Direct link to What Are Embeddings?" title="Direct link to What Are Embeddings?">​</a></h2>
<p>Embeddings are a way to represent data as vectors, where similar items are positioned close to one another in a multidimensional space. They are widely used in areas like NLP to capture relationships between words, images, or sounds more effectively than simpler methods like One-Hot Encoding.</p>
<p>These embeddings allow data with similar characteristics to cluster together, which makes similarity search and grouping of related information much more efficient.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a-simple-analogy-colors-in-a-3-d-vector-space">A Simple Analogy: Colors in a 3-D Vector Space<a href="#a-simple-analogy-colors-in-a-3-d-vector-space" class="hash-link" aria-label="Direct link to A Simple Analogy: Colors in a 3-D Vector Space" title="Direct link to A Simple Analogy: Colors in a 3-D Vector Space">​</a></h3>
<p>To make this easier to understand, let’s consider a familiar example: colors in the <strong>RGBA</strong> model. Each color is represented by four values — the intensities of red, green, and blue, plus an alpha channel that controls transparency — placing it in a <strong>four-dimensional (4D)</strong> space.</p>
<blockquote>
<p>Since we can’t visualize four dimensions directly, we simplify by plotting just the RGB components in a 3D space. The alpha value still exists, but isn&#x27;t represented visually in this view.</p>
</blockquote>
<p>For example:</p>
<ul>
<li><strong>Red:</strong> <code>[255, 0, 0, 1]</code></li>
<li><strong>Light Red:</strong> <code>[255, 0, 0, 0.5]</code></li>
<li><strong>Green:</strong> <code>[0, 255, 0, 1]</code></li>
<li><strong>Blue:</strong> <code>[0, 0, 255, 1]</code></li>
</ul>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAIAAADzBuo/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAATUlEQVR4nF2NSQ7AIAwD+f9rEQdqSBxXgVZdfLHkGclFkiIkzdZ6rU7GSu5S2ZUOQGCgAzCzP05DweB7+eD9M53ul5R4GPsBs3kbz/cJ4fiUbrbbERoAAAAASUVORK5CYII=&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="299"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/image-5.a8a5980.640.png" srcset="/assets/ideal-img/image-5.a8a5980.640.png 640w,/assets/ideal-img/image-5.79d1e13.920.png 920w,/assets/ideal-img/image-5.6cc8b79.1166.png 1166w" width="640" height="299"></noscript></div>
<br>
<p>When we plot these colors in a 3D space using only the RGB components, we can clearly see that <strong>Red</strong> and <strong>Light Red</strong> appear close to each other, while <strong>Green</strong> and <strong>Blue</strong> are located farther away. This spatial proximity reflects how similar the colors are — in this case, due to their shared red intensity.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="representing-words-with-embeddings">Representing Words with Embeddings<a href="#representing-words-with-embeddings" class="hash-link" aria-label="Direct link to Representing Words with Embeddings" title="Direct link to Representing Words with Embeddings">​</a></h3>
<p>Now, let’s apply this idea to how we represent words. Instead of using three or four dimensions like in the RGB color model, words are typically represented in <strong>vector spaces with hundreds or even thousands of dimensions</strong>. Each dimension captures some aspect or feature of the word — such as its meaning, context, or relationships with other words.</p>
<p>For example:</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAECAIAAAA4WjmaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAARUlEQVR4nE2KsQ3AIBAD2X/GLIAUgUJBSP5tIwpeXOc7Jx3QTeRpkiS4AZDk/nN8I2ffp5W5B8Fe3t5amJUDGO/rKbVGnsBadlH2AdzdAAAAAElFTkSuQmCC&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="254"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/image-6.0b60299.640.png" srcset="/assets/ideal-img/image-6.0b60299.640.png 640w,/assets/ideal-img/image-6.9efa731.920.png 920w,/assets/ideal-img/image-6.e50d853.1166.png 1166w" width="640" height="254"></noscript></div>
<br>
<p>In the example above, <strong>&quot;dog&quot;</strong> and <strong>&quot;cat&quot;</strong> appear close to each other in the vector space, because both are pets and share contextual similarities. On the other hand, <strong>&quot;car&quot;</strong> and <strong>&quot;forest&quot;</strong> are positioned farther away due to their very different meanings and usage patterns.</p>
<blockquote>
<p>Illustrative only — no actual model was used to generate this plot.</p>
</blockquote>
<p>These <strong>high-dimensional vector spaces</strong> are generated by <strong>embedding models</strong>, which analyze large volumes of text and learn to position words based on the contexts in which they appear. The closer two words are in the space, the more semantically similar they are likely to be.</p>
<p>This representation makes it possible for machines to reason about meaning — not just recognize exact matches, but <strong>understand relatedness</strong>. That’s a key foundation for modern AI tasks like question answering, translation, and semantic search.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="explore-a-real-embedding-representation">Explore a Real Embedding Representation<a href="#explore-a-real-embedding-representation" class="hash-link" aria-label="Direct link to Explore a Real Embedding Representation" title="Direct link to Explore a Real Embedding Representation">​</a></h3>
<p>If you’d like to see what embeddings look like in practice, you can explore a live, interactive visualization using the <a href="https://projector.tensorflow.org/" target="_blank" rel="noopener noreferrer">TensorFlow Embedding Projector</a>. This tool allows you to <strong>navigate through high-dimensional vector spaces</strong> and observe how words, images, or other data points are organized based on their semantic relationships.</p>
<p>You&#x27;ll see something similar to the example below, where each word is plotted in a space with up to 200 dimensions, reduced visually to 2D or 3D using dimensionality reduction techniques like <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener noreferrer">PCA</a> ou <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank" rel="noopener noreferrer">t-SNE</a>.</p>
<div style="background-size:cover;background-repeat:no-repeat;position:relative;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAGCAIAAAB1kpiRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAsklEQVR4nAXB226DIBgAYN7/JXq5N1ja3SyLbdKlox4C/KA/KIKoxR3Utskuuuz7yOPxN3+Pn1p21nb9YA0gSufcJX5NQZJ1WYZh/InR2db7oPipSA/eh1JBjBNZbr+Yb1t407VTSgpG6SmpsK5kbuCdlHv2unnytjBYlUoJAC448EIyyrMj6bWvWeND59tGgEzPZ5YmL7vnCrVBRa73dZ37PnQ1Cq0NCC54QT8oyqxt9D/eTJqo3IBxpgAAAABJRU5ErkJggg==&quot;)"><svg style="width:100%;height:auto;max-width:100%;margin-bottom:-4px" width="640" height="393"></svg><noscript><img style="width:100%;height:auto;max-width:100%;margin-bottom:-4px;position:absolute;top:0;left:0" src="/assets/ideal-img/image-7.af13a16.640.png" srcset="/assets/ideal-img/image-7.af13a16.640.png 640w,/assets/ideal-img/image-7.32a0184.902.png 902w" width="640" height="393"></noscript></div>
<br>
<p>In this visualization, each dot represents a word, and the proximity between them reflects how similar their meanings are according to the embedding model. For example, the word <strong>“store”</strong> is surrounded by words like <strong>“shop,” “market,”</strong> and <strong>“retail”</strong>, indicating that the model has learned their contextual similarity from large-scale text data.</p>
<blockquote>
<p>This type of embedding space enables machines not only to recognize individual terms, but also to understand relationships between them — a powerful capability for tasks like semantic search, where the goal is to find relevant information based on meaning rather than exact matches.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-vector-db-works">How Does a Vector Database Work?<a href="#how-vector-db-works" class="hash-link" aria-label="Direct link to How Does a Vector Database Work?" title="Direct link to How Does a Vector Database Work?">​</a></h2>
<p>Now that we’ve covered the basic concepts, we can better understand how <strong>vector databases</strong> operate. A vector database organizes data in the form of vectors, which occupy positions in <strong>multidimensional space</strong>. The main goal is to enable <strong>semantic search</strong> — that is, finding similar items based on their <strong>vector proximity</strong>, rather than exact matches like in traditional databases.</p>
<p>For example, imagine a database of images where each image is represented as a vector that captures its visual characteristics — such as color, shape, and texture. If you want to find images similar to a specific picture of a cat, the vector database will compute the <strong>distance</strong> between the query image&#x27;s vector and the vectors of other stored images. Those with the <strong>shortest distance</strong> will be returned as results, since they share similar characteristics.</p>
<p>A common real-world application of this is <strong>facial recognition</strong>, where the similarity between the vector of a captured face and that of a registered face can indicate a potential match.</p>
<p>But how exactly is similarity between vectors measured?</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vector-distance-and-similarity-search">Vector Distance and Similarity Search<a href="#vector-distance-and-similarity-search" class="hash-link" aria-label="Direct link to Vector Distance and Similarity Search" title="Direct link to Vector Distance and Similarity Search">​</a></h3>
<p>In a vector database, items with similar features are clustered close to each other in a <strong>multidimensional space</strong>. The &quot;closeness&quot; — or <strong>similarity</strong> — between items is determined by specific <strong>distance metrics</strong>, such as <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="noopener noreferrer"><strong>Euclidean distance</strong></a> or <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener noreferrer"><strong>cosine similarity</strong></a>.</p>
<p>In the case of word searches, like in our earlier embeddings example, a vector database can return words that are semantically close to the query. For instance, if you search for the word <strong>&quot;feline&quot;</strong>, the database might return <strong>&quot;cat,&quot; &quot;tiger,&quot;</strong> or <strong>&quot;leopard&quot;</strong>, based on how near their vectors are in the semantic space.</p>
<blockquote>
<p>This kind of vector-based matching allows systems to go beyond surface-level keywords and find results that are <strong>meaningfully related</strong>, even when the exact words don’t match — a major advantage in modern AI applications.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="applications-of-vector-databases">Applications of Vector Databases<a href="#applications-of-vector-databases" class="hash-link" aria-label="Direct link to Applications of Vector Databases" title="Direct link to Applications of Vector Databases">​</a></h2>
<p>Vector databases are already being widely used to solve challenges related to <strong>unstructured data</strong> across various domains. Here are some practical examples:</p>
<ul>
<li>
<p><strong>Image Search</strong>: Medical systems and platforms like Google Images use vector databases to find visually similar images based on features like color, shape, and texture — which helps with diagnoses and content discovery.</p>
</li>
<li>
<p><strong>Product Recommendations</strong>: In e-commerce, vector databases suggest products based on past searches or user purchase history, enabling a more personalized shopping experience.</p>
</li>
<li>
<p><strong>Facial Recognition</strong>: Security systems use vector databases to compare facial images, identifying matches with high precision — making authentication and surveillance more effective.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation" target="_blank" rel="noopener noreferrer"><strong>RAG (Retrieval-Augmented Generation)</strong></a>: This technique combines document retrieval from vector databases with the response generation capabilities of large language models (LLMs). It allows these models to specialize in specific topics without requiring retraining.</p>
</li>
</ul>
<blockquote>
<p><em>In my <a href="https://www.linkedin.com/posts/tchez_jornadadeiniciaaexaetocientaedfica-praeamiojovempesquisador-activity-7263597654570369024-2PqS?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAADUhp3MBjeUrhJg0P5LSvpRa8yf14r7iP3Y" target="_blank" rel="noopener noreferrer">undergraduate thesis</a>, I used RAG to build a chatbot focused on mental health. The system retrieves relevant information and provides more accurate and contextualized responses.</em></p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>As unstructured data continues to grow exponentially, <strong>vector databases are becoming essential for retrieving information efficiently and meaningfully</strong>. By enabling <strong>similarity-based search</strong> rather than relying on exact matches, they open up new possibilities for interacting with complex data in more intuitive and intelligent ways.</p>
<p>From applications like <strong>image search</strong>, <strong>product recommendations</strong>, and <strong>facial recognition</strong>, to powering advanced techniques such as <strong>RAG</strong>, vector databases are poised to play a foundational role in the future of artificial intelligence. They help bridge the gap between raw data and actionable knowledge — fueling innovation across industries.</p>
<p>In a world where context and meaning matter more than ever, vector databases offer a paradigm shift. By interpreting data <strong>semantically</strong>, they have the potential to <strong>revolutionize the way we store, retrieve, and understand information</strong> — connecting today’s digital systems with the knowledge-driven technologies of tomorrow.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><strong>Links</strong></summary><div><div class="collapsibleContent_i85q"><ul>
<li><a href="https://www.oracle.com/br/big-data/what-is-big-data" target="_blank" rel="noopener noreferrer">What is Big Data? – Oracle</a></li>
<li><a href="https://www.oracle.com/br/database/what-is-database" target="_blank" rel="noopener noreferrer">What is a Database? – Oracle</a></li>
<li><a href="https://aws.amazon.com/pt/what-is/database" target="_blank" rel="noopener noreferrer">What is a Database? – AWS</a></li>
<li><a href="https://aws.amazon.com/pt/what-is/vector-databases" target="_blank" rel="noopener noreferrer">What is a Vector Database? – AWS</a></li>
<li><a href="https://www.elastic.co/pt/what-is/vector-embedding" target="_blank" rel="noopener noreferrer">What is a Vector Embedding? – Elastic</a></li>
<li><a href="https://medium.com/turing-talks/word-embedding-fazendo-o-computador-entender-o-significado-das-palavras-92fe22745057" target="_blank" rel="noopener noreferrer">Word Embedding: Making Computers Understand Word Meaning – Turing Talks</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/embeddings?hl=pt-br" target="_blank" rel="noopener noreferrer">Embeddings – Google Machine Learning Crash Course</a></li>
<li><a href="https://aws.amazon.com/pt/what-is/retrieval-augmented-generation" target="_blank" rel="noopener noreferrer">What is Retrieval-Augmented Generation (RAG)? – AWS</a></li>
</ul></div></div></details></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/rag/">rag</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai/">ai</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/programming/">programming</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/article/">article</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/tchez/brain-blog/edit/main/blog/2025/06/12-vector-database.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/dunder-methods/"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Do you know what magic methods are in Python? Hint: You use them every day!</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#from-data-to-knowledge-the-power-of-vector-databases" class="table-of-contents__link toc-highlight">From Data to Knowledge: The Power of Vector Databases</a><ul><li><a href="#the-rise-of-unstructured-data" class="table-of-contents__link toc-highlight">The Rise of Unstructured Data</a></li></ul></li><li><a href="#what-is-a-database" class="table-of-contents__link toc-highlight">What Is a Database?</a><ul><li><a href="#relational-databases" class="table-of-contents__link toc-highlight">Relational Databases</a></li><li><a href="#non-relational-databases-nosql" class="table-of-contents__link toc-highlight">Non-Relational Databases (NoSQL)</a></li><li><a href="#vector-databases" class="table-of-contents__link toc-highlight">Vector Databases</a></li></ul></li><li><a href="#what-is-a-vector" class="table-of-contents__link toc-highlight">What Is a Vector?</a><ul><li><a href="#representing-words-with-vectors" class="table-of-contents__link toc-highlight">Representing Words with Vectors</a></li><li><a href="#one-hot-encoding" class="table-of-contents__link toc-highlight">One-Hot Encoding</a></li><li><a href="#the-limitations-of-one-hot-encoding" class="table-of-contents__link toc-highlight">The Limitations of One-Hot Encoding</a></li><li><a href="#advanced-representations-embeddings" class="table-of-contents__link toc-highlight">Advanced Representations: Embeddings</a></li></ul></li><li><a href="#what-are-embeddings" class="table-of-contents__link toc-highlight">What Are Embeddings?</a><ul><li><a href="#a-simple-analogy-colors-in-a-3-d-vector-space" class="table-of-contents__link toc-highlight">A Simple Analogy: Colors in a 3-D Vector Space</a></li><li><a href="#representing-words-with-embeddings" class="table-of-contents__link toc-highlight">Representing Words with Embeddings</a></li><li><a href="#explore-a-real-embedding-representation" class="table-of-contents__link toc-highlight">Explore a Real Embedding Representation</a></li></ul></li><li><a href="#how-vector-db-works" class="table-of-contents__link toc-highlight">How Does a Vector Database Work?</a><ul><li><a href="#vector-distance-and-similarity-search" class="table-of-contents__link toc-highlight">Vector Distance and Similarity Search</a></li></ul></li><li><a href="#applications-of-vector-databases" class="table-of-contents__link toc-highlight">Applications of Vector Databases</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></div></div>
</body>
</html>